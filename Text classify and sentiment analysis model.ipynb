{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from snownlp import SnowNLP\n",
    "from snownlp import sentiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/jieba/stops.txt', 'r', encoding='utf8') as f:  # 中文的停用字\n",
    "    stops = f.read().split('\\n') \n",
    "    \n",
    "#增加用詞\n",
    "s = ['沒','都','沒有','很','不','不要','請','好','嗎','因為','希望','熊貓','真的','然後','圖檔','無法','今天','已經','請問','一起','可不可以','�',\\\n",
    "     '再','不是','什麼','不會','不能','這個','還是','一樣','一個','到底','非常','一次','一點','一堆','不用','常常','一下','給我','明明','怎麼','這次'\\\n",
    "    '還沒','這麼','這是','還有','根本','這位','現在','以後','怎樣','之後','每次','是不是','一定','應該','越來越','太']\n",
    "for w in s:\n",
    "    stops.append(w)\n",
    "   \n",
    "\n",
    "#載入訓練資料庫\n",
    "df_train=pd.read_excel(\"/content/drive/My Drive/Colab Notebooks/Python/Tools/train_2.0.xlsx\")\n",
    "\n",
    "#斷詞並刪除停用詞\n",
    "def cut(mytext):\n",
    "    terms = [t for t in jieba.cut(mytext) if t not in stops]\n",
    "    return \" \".join(terms)\n",
    "#斷詞\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "\n",
    "#拆解評論內容\n",
    "df_train['comment'] = df_train.內容\n",
    "\n",
    "#模型用訓練資料\n",
    "X=df_train['comment']\n",
    "y=df_train['分類']\n",
    "\n",
    "#文字量化模型\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# feature_extractor = TfidfVectorizer(\n",
    "#                        token_pattern=r'(?u)\\b\\w+\\b',max_df = 0.6)\n",
    "feature_extractor = CountVectorizer(\n",
    "            analyzer=\"word\", ngram_range=(1, 2), binary=True,\n",
    "            token_pattern=r'([a-zA-Z]+|\\w)')\n",
    "X1 = feature_extractor.fit_transform(X)\n",
    "\n",
    "#分類模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "INTENT_CLASSIFY_REGULARIZATION = \"l2\"\n",
    "\n",
    "lr = LogisticRegression(penalty=INTENT_CLASSIFY_REGULARIZATION,\n",
    "             class_weight='balanced',max_iter=1000)\n",
    "lr.fit(X1, y)\n",
    "\n",
    "#分類功能\n",
    "def category(text):\n",
    "    text=[text]\n",
    "    X = feature_extractor.transform(text)\n",
    "    category = lr.predict(X)\n",
    "    return ''.join(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_cn(text):\n",
    "    s = SnowNLP(text)\n",
    "    return s.sentiments\n",
    "\n",
    "def pos_neg(text):\n",
    "    s = SnowNLP(text)\n",
    "    if s.sentiments > 0.6:\n",
    "        return('正面')\n",
    "    elif 0.4 <= s.sentiments <= 0.6:\n",
    "        return('中立')\n",
    "    else:\n",
    "        return('負面')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
